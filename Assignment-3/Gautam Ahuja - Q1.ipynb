{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Gautam Ahuja\n",
    "Introduction to Machine Learning\n",
    "Raghavendra Singh\n",
    "Assignment 3 Code & Report\n",
    "Implement PCA using Eigen library\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement  PCA in python (10 marks) using https://github.com/stack-of-tasks/eigenpy\n",
    "\n",
    "Generate random datasets with (m, n ) = [(100,100), (1000,1000), (10000, 10000), (10000,50000), (50000,50000)]\n",
    "and do PCA with number of components = 0.1*n\n",
    "\n",
    "Report run time of your implementation and compare with runtime of sklearn.decomposition.pca\n",
    "\n",
    "\n",
    "Submit code and method for generating dataset along with above report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eigenpy import quaternion\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import eigenpy\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "To perform PCA, we need to perform the following steps:\n",
    "1. Standardize the data. Mean of each feature should be 0 and standard deviation should be 1. The standardization is done along each row.\n",
    "2. Calculate the covariance matrix of the standardized data.\n",
    "3. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. Sort eigenvalues in descending order and choose the top k eigenvectors that correspond to the k largest eigenvalues. k is the number of dimensions of the new feature subspace (k<=n).\n",
    "5. Construct the projection matrix W from the selected k eigenvectors.\n",
    "6. Transform the original dataset X via W to obtain a k-dimensional feature subspace Y.\n",
    "\n",
    "We will use the eigenpy library to perform the PCA from scratch. The eigenpy library is a python wrapper for the Eigen library. The Eigen library is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. It supports all matrix sizes, from small fixed-size matrices to arbitrarily large dense matrices, and even sparse matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "We generate the datasets using the make_classification function from sklearn -- this creates mulclass dataset by allocating each classs one or more normally distributed clusters of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (100, 100)\n",
      "Dataset shape:  (1000, 1000)\n",
      "Dataset shape:  (10000, 10000)\n",
      "Dataset shape:  (10000, 50000)\n",
      "Dataset shape:  (50000, 50000)\n"
     ]
    }
   ],
   "source": [
    "# Generate random datasets with (m, n ) = [(100,100), (1000,1000), (10000, 10000), (10000,50000), (50000,50000)]\n",
    "dataset = []\n",
    "np.random.seed(42)\n",
    "m = [100, 1000, 10000, 10000, 50000]\n",
    "n = [100, 1000, 10000, 50000, 50000]\n",
    "for i in range(len(m)):\n",
    "    # dataset.append(np.random.rand(m[i], n[i]))\n",
    "    dataset.append(make_classification(n_samples=m[i], n_features=n[i], random_state=42))\n",
    "    print(\"Dataset shape: \", dataset[i].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "The (10000, 50000) and (50000, 50000) dataset is too large -- 18 GB -- and cannot be allocated in memory\n",
    "\n",
    "It gives the following error:\n",
    "\n",
    "```MemoryError: Unable to allocate 18.6 GiB for an array with shape (50000, 50000) and data type float64```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA with number of components = 0.1*n\n",
    "n_components = [int(0.1 * n[i]) for i in range(len(n))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenpy has the following functions available:\n",
    "1. AngleAxis: Represents a rotation by an angle around an axis.\n",
    "2. ComputationInfo: Enumerates the possible outcomes of a linear algebra computation.\n",
    "3. DecompositionOptions: Enumerates the possible options for a decomposition.\n",
    "4. EigenSolver: Computes eigenvalues and eigenvectors of a matrix.\n",
    "5. Exception: Base class for all exceptions thrown by EigenPy.\n",
    "6. LDLT: Computes the LDLT decomposition of a matrix with partial pivoting.\n",
    "7. LLT: Computes the LLT decomposition of a matrix.\n",
    "8. MINRES: Computes the MINRES decomposition of a matrix.\n",
    "9. Quaternion: Represents a rotation by a quaternion.\n",
    "10. SelfAdjointEigenSolver: Computes eigenvalues and eigenvectors of a selfadjoint matrix.\n",
    "11. SimdInstructionSetsInUse: Enumerates the SIMD instruction sets in use.\n",
    "12. StdVec_MatrixXd: A std::vector of Eigen::MatrixXd.\n",
    "13. StdVec_MatrixXi: A std::vector of Eigen::MatrixXi.\n",
    "14. StdVec_VectorXd: A std::vector of Eigen::VectorXd.\n",
    "15. StdVec_VectorXi: A std::vector of Eigen::VectorXi.\n",
    "16. checkVersionAtLeast: Checks that the version of EigenPy is at least the given version.\n",
    "17. fromEulerAngles: Constructs a rotation from Euler angles.\n",
    "18. is_approx: Returns true if the two values are approximately equal.\n",
    "19. seed: Seeds the random number generator.\n",
    "20. sharedMemory: Returns the shared memory segment used by EigenPy.\n",
    "21. solvers: Returns the list of available solvers.\n",
    "22. toEulerAngles: Returns the Euler angles of a rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(dataset, n_components):\n",
    "    # Standardize the dataset along rows\n",
    "    dataset = (dataset - np.mean(dataset, axis=0))# / np.std(dataset, axis=0)\n",
    "    # Compute the covariance matrix\n",
    "    cov = np.cov(dataset.T)\n",
    "    # Compute the eigenvalues and eigenvectors of the covariance matrix\n",
    "    eig_vals = eigenpy.EigenSolver(cov).eigenvalues()\n",
    "    eig_vecs = eigenpy.EigenSolver(cov).eigenvectors()\n",
    "    \n",
    "    # Sort the eigenvalues in descending order\n",
    "    idx = eig_vals.argsort()[::-1]\n",
    "    \n",
    "    # idx = np.arange(0,len(eig_vals), 1)\n",
    "    # idx = ([x for _,x in sorted(zip(eig_vals, idx))])[::-1]\n",
    "    # Sort the eigenvectors according to the sorted eigenvalues\n",
    "    eig_vals = eig_vals[idx]\n",
    "    eig_vecs = eig_vecs[:, idx]\n",
    "    # Compute the projection matrix\n",
    "    proj_mat = eig_vecs[:, :n_components]\n",
    "    # Project the dataset onto the projection matrix\n",
    "    dataset = np.dot(dataset, proj_mat)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (100, 100)\n",
      "Reduced dataset shape:  (100, 10)\n",
      "Time taken:  0.019929885864257812\n",
      "--------------------------------------------------\n",
      "Dataset shape:  (1000, 1000)\n",
      "Reduced dataset shape:  (1000, 100)\n",
      "Time taken:  6.69486665725708\n",
      "--------------------------------------------------\n",
      "Total time taken:  6.715179443359375\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(2):#len(dataset)):\n",
    "    print(\"Dataset shape: \", dataset[i].shape)\n",
    "    curr_time = time.time()\n",
    "    reduced_dataset =  pca(dataset[i], n_components[i])\n",
    "    print(\"Reduced dataset shape: \", reduced_dataset.shape)\n",
    "    print(\"Time taken: \", time.time() - curr_time)\n",
    "    print(\"--------------------------------------------------\")\n",
    "print(\"Total time taken: \", time.time() - start_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Using Sklearn\n",
    "Now we implement PCA using the sklearn library and note the time taken for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (100, 100)\n",
      "Reduced dataset shape:  (10,)\n",
      "Time taken:  0.0053288936614990234\n",
      "--------------------------------------------------\n",
      "Dataset shape:  (1000, 1000)\n",
      "Reduced dataset shape:  (100,)\n",
      "Time taken:  1.2234978675842285\n",
      "--------------------------------------------------\n",
      "Total time taken:  1.229102373123169\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(2):#len(dataset)):\n",
    "    print(\"Dataset shape: \", dataset[i].shape)\n",
    "    curr_time = time.time()\n",
    "    pca = PCA(n_components=n_components[i])\n",
    "    reduced_dataset = pca.fit_transform(dataset[i])\n",
    "    print(\"Reduced dataset shape: \", reduced_dataset[i].shape)\n",
    "    print(\"Time taken: \", time.time() - curr_time)\n",
    "    print(\"--------------------------------------------------\")\n",
    "print(\"Total time taken: \", time.time() - start_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Measurement\n",
    "We use the time library to measure the time taken for the PCA implementation using eigenpy and sklearn.\n",
    "\n",
    "### Using eigenpy\n",
    "Time taken for:\n",
    "1. 100 x 100 dataset: 0.019929885864257812 seconds\n",
    "2. 1000 x 1000 dataset: 6.69486665725708 seconds\n",
    "\n",
    "### Using sklearn\n",
    "Time taken for:\n",
    "1. 100 x 100 dataset: 0.0053288936614990234 seconds\n",
    "2. 1000 x 1000 dataset: 1.2234978675842285 seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "I could not run the code for the (10000, 50000) and (50000, 50000) dataset due to memory constraints. I have run the code for the other datasets and have included the results in the report.\n",
    "\n",
    "I could not run the PCA on (10000, 10000), (10000, 50000) and (50000, 50000) datasets due to memory and CPU constraints. I have run the code for the other datasets and have included the results in the report. Running on these always resulted in a Kernel Crash. My task manager showed that the memory usage was 100% and the CPU usage was 100% as well. I tried running the code on Google Colab as well, but the eigenpy library was not available there as it is only available for conda environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
